{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a06360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook implements all the necessary functions\n",
    "#for SC-WEAT-WEAT analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefb6cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilayd\\AppData\\Local\\Temp\\ipykernel_20920\\354234303.py:2: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file='glove.42B.300d.txt', word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
      "2022-08-15 20:09:59,056 - glove2word2vec - INFO - running C:\\Users\\ilayd\\IDP\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py --input glove.42B.300d.txt --output glove.42B.300d.w2vformat.txt\n",
      "C:\\Users\\ilayd\\IDP\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py:125: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  num_lines, num_dims = glove2word2vec(args.input, args.output)\n",
      "2022-08-15 20:09:59,056 - keyedvectors - INFO - loading projection weights from glove.42B.300d.txt\n",
      "2022-08-15 20:17:56,732 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (1917494, 300) matrix of type float32 from glove.42B.300d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-08-15T20:17:56.718805', 'gensim': '4.2.0', 'python': '3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'load_word2vec_format'}\n",
      "2022-08-15 20:17:56,732 - glove2word2vec - INFO - converting 1917494 vectors from glove.42B.300d.txt to glove.42B.300d.w2vformat.txt\n",
      "2022-08-15 20:17:58,555 - keyedvectors - INFO - storing 1917494x300 projection weights into glove.42B.300d.w2vformat.txt\n",
      "2022-08-15 20:27:27,414 - glove2word2vec - INFO - Converted model with 1917494 vectors and 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#we run it only once, it takes around half an hour. We do this part to convert common crawl vectors into the format we can use.\n",
    "#aftre that we store the result as a model that we can use for common crawl\n",
    "#glove2word2vec(glove_input_file='glove.42B.300d.txt', word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "common_crawl2 = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99b6997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the notebook downloads and uploads all the word embedding models\n",
    "#if you receive an error such as: ... not found, please try to install the library\n",
    "#by following pip install ...\n",
    "import math\n",
    "from scipy import spatial\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#import streamlit as st\n",
    "import plotly.graph_objs as go\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#this is the first common_crawl we worked on, it didnt give correct results\n",
    "#common_crawl = KeyedVectors.load_word2vec_format('crawl-300d-2M.vec', binary=False)\n",
    "glove_vectors_google_news = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_vectors_twitter = gensim.downloader.load('glove-twitter-200')\n",
    "glove_vectors_wiki = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "glove_vectors_wiki3 = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "coha_model_1960 = KeyedVectors.load_word2vec_format('1990.txt', binary=False, unicode_errors='replace')\n",
    "\n",
    "#structure of the coha pretrained word embeddings are different than the other embeddings,\n",
    "#it requires some manual preprocessing to be able to reach the embeddings from words\n",
    "\n",
    "coha_corpus_1960_list  = []\n",
    "for idx, val in enumerate(coha_model_1960.index_to_key):\n",
    "    just_word = val.split('_')\n",
    "    word_key = just_word[0]\n",
    "    coha_corpus_1960_list.append(word_key)\n",
    "\n",
    "coha_list = coha_corpus_1960_list\n",
    "coha_model = coha_model_1960\n",
    "\n",
    "#####\n",
    "male_group_words = ['he', 'son', 'his', 'him', 'father', 'man', 'boy', 'himself', 'male', 'brother', 'sons', 'fathers',\n",
    "                    'men', 'boys', 'males', 'brothers', 'uncle',\n",
    "                    'uncles', 'nephew', 'nephews']\n",
    "female_group_words = [\"she\", 'daughter', 'hers', 'her', 'mother', 'woman', 'girl', 'herself', 'female', 'sister',\n",
    "                      'daughters', 'mothers', 'women',\n",
    "                      'girls', 'femen', 'sisters', 'aunt', 'aunts', 'niece', 'nieces']\n",
    "\n",
    "#change this parameter according to the model you want to use for calculations, putting\n",
    "#a '#' symbol in front of the line, skips that line, for example, if we want \n",
    "#to use twitter embeddings we should put # in front of every other embedding = ... line\n",
    "\n",
    "embedding = glove_vectors_twitter\n",
    "#embedding = glove_vectors_google_news\n",
    "#embedding = glove_vectors_wiki\n",
    "#embedding = common_crawl2\n",
    "#embedding = coha_model_1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b1a9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a word in the word list doesnt exist in the provided embedding,\n",
    "#we just skip that word, this function implements that\n",
    "\n",
    "def collect_existing_words(word_list, embedding):\n",
    "    lst = []\n",
    "    if embedding == coha_model:\n",
    "        for words in word_list:\n",
    "            existed_words = []\n",
    "            for word in words:\n",
    "                if word in coha_list:\n",
    "                    idx = coha_list.index(word)\n",
    "                    existed_words.append(embedding.index_to_key[idx])\n",
    "                else:\n",
    "                    pass\n",
    "            lst.append(existed_words)\n",
    "    else:\n",
    "        for words in word_list:\n",
    "            existed_words = []\n",
    "            for word in words:\n",
    "                if word in embedding.key_to_index:\n",
    "                    existed_words.append(word)\n",
    "                else:\n",
    "                    pass\n",
    "            lst.append(existed_words)\n",
    "    res = list(filter(None, lst))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a43596a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates the cosine similarity between two vectors\n",
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7e19fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stop words from a word phrase before we collect its embedding\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44cd086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing stop words from the given word list, more generalized version of\n",
    "#remove_Stopwords function\n",
    "\n",
    "def clean_data(word_list):\n",
    "    clean_word_list = []\n",
    "    for wrds in word_list:\n",
    "        filtered_text = remove_stopwords(wrds)\n",
    "        clean_word_list.append(filtered_text)\n",
    "        \n",
    "    return clean_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79026026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting words' corresponding word embeddings and putting them in an array,\n",
    "#each row of the array represents one word embedding\n",
    "\n",
    "def collect_word_embeds(word_list,embedding):\n",
    "    word_vec = []\n",
    "    lst = collect_existing_words(word_list, embedding)\n",
    "    for words in lst:\n",
    "        vec = 0\n",
    "        for word in words:\n",
    "            vect = embedding[word]\n",
    "            vec += vect\n",
    "        vect2 = vec / len(words)\n",
    "        word_vec.append(vect2)\n",
    "        \n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30c0f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another approach: this time we have the list of personality traits from different sources\n",
    "#(for example one is http://ideonomy.mit.edu/essays/traits.html),\n",
    "#we calculate their similarity with entrepreneurship according to cosine similarity\n",
    "#and take the words that are more similar\n",
    "\n",
    "#this function returns the words that are most similar(top n) and their corresponding\n",
    "#word embeddings\n",
    "\n",
    "def collect_predefined_list_embeddings(wordlist_file, embedding, entrepreneurship_word, top_n):\n",
    "    words = []\n",
    "    with open(wordlist_file) as f:\n",
    "        for line in f:\n",
    "            words.append(line[:-1].lower())\n",
    "            \n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in embedding.key_to_index:\n",
    "            word_dict[word] = embedding.similarity(entrepreneurship_word, word)\n",
    "    word_dict2 = sorted(word_dict.items(), key=lambda x:x[1], reverse = True)\n",
    "    new_words = []\n",
    "    for word,score in word_dict2[:top_n]:\n",
    "        new_words.append(word)\n",
    "    word_embeds = collect_word_embeds(new_words, embedding)\n",
    "    return new_words, word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad97e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating sc-weat\n",
    "#X: female group words\n",
    "#Y: male group words\n",
    "#A: attribute group words\n",
    "#w: one word embedding\n",
    "\n",
    "#this function calculates the cosine similarity between a given word vector\n",
    "#and each attribute word in attribute word list, and returns the avg of the\n",
    "#results. For example we have the word she as one word from group words (w), and \n",
    "#we have the home attribute word list(A). What we do is calculating the similarity \n",
    "#between she and each word in the attribute list(A) and returning the average of\n",
    "#the calculation. Result is the similarity between the word she(w), and the attribute\n",
    "#home(which is a list of words, A)\n",
    "\n",
    "def s(w, A):\n",
    "    #the lines below calculates the cosine similarity of the for w for each element\n",
    "    #(a) of the attribute word list(A), then returns the mean of this calculation as\n",
    "    #result.\n",
    "    \n",
    "    a_cos = np.array([cossim(w, a) for a in A])\n",
    "    return (np.mean(a_cos))\n",
    "\n",
    "#This function calculates the SC-WEAT score by using the function s(w,A). X \n",
    "#represents all female group words. We use the function s to calculate\n",
    "#the cosine similarity for each word(x) of the female group list(X). \n",
    "#We do the same thing for each word(y) of the male group list(Y).\n",
    "#SC-weat score is calculated by substracting the average results(female - male)\n",
    "#then we divide it to standard deviation to return final SC-weat score.\n",
    "\n",
    "def sc_weat_effect_size(X, Y, A):\n",
    "    x_s = np.array([s(x, A) for x in X])\n",
    "    y_s = np.array([s(y, A) for y in Y])\n",
    "\n",
    "    return (np.mean(x_s) - np.mean(y_s)) / np.std(np.concatenate((x_s, y_s)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6948cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these functions below, uses the weight approach introduced in papers but since\n",
    "#our group word list mostly have the same number of the elements(especially when\n",
    "#we collect top 50 closest words to represent female or male, both lists has 50\n",
    "#elements, there is no difference between sc_weat_effect_size method in terms\n",
    "#of results.)\n",
    "\n",
    "def weighted_std(values, weights):\n",
    "\t\"\"\"\n",
    "\tReturn the weighted standard deviation.\n",
    "\n",
    "\tvalues, weights -- Numpy ndarrays with the same shape.\n",
    "\t\"\"\"\n",
    "\taverage = np.average(values, weights=weights)\n",
    "\t# Fast and numerically precise:\n",
    "\tvariance = np.average((values-average)**2, weights=weights)\n",
    "\t# Small sample size bias correction:\n",
    "\tvariance_ddof1 = variance*len(values)/(len(values)-1)\n",
    "\treturn math.sqrt(variance_ddof1)\n",
    "\n",
    "def diff_sim(X, A, B):\n",
    "\n",
    "\t\tsum_A = 0\n",
    "\t\tsum_B = 0\n",
    "\n",
    "\t\tall_sims = []\n",
    "\t\tfor a in A:\n",
    "\t\t\ta_ = a.reshape(1, -1)\n",
    "\t\t\tresults = spatial.distance.cdist(a_, X, 'cosine')\n",
    "\t\t\tsum_X = (1 - results).sum()\n",
    "\t\t\tval = sum_X/len(X)\n",
    "\t\t\tsum_A += val\n",
    "\t\t\tall_sims.append(val)\n",
    "\t\tave_A = sum_A/len(A)\n",
    "\n",
    "\t\tfor b in B:\n",
    "\t\t\tb_ = b.reshape(1, -1)\n",
    "\t\t\tresults = spatial.distance.cdist(b_, X, 'cosine')\n",
    "\t\t\tsum_X = (1 - results).sum()\n",
    "\t\t\tval = sum_X/len(X)\n",
    "\t\t\tsum_B += val\n",
    "\t\t\tall_sims.append(val)\n",
    "\t\tave_B = sum_B/len(B)\n",
    "\n",
    "\t\tdifference = ave_A - ave_B\n",
    "\n",
    "\t\t# For SD calculation, assign weights based on frequency of opposite category\n",
    "\t\tweights = [len(B) for num in range(len(A))] + [len(A) for num in range(len(B))]\n",
    "\t\tstandard_dev = weighted_std(all_sims, weights)\n",
    "\t\teffect_size = difference/standard_dev\n",
    "\n",
    "\t\treturn effect_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a65fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for calculating the p_value\n",
    "\n",
    "import random\n",
    "def within_group_cohesion(X):\n",
    "\tdist = spatial.distance.pdist(X, 'cosine')\n",
    "\treturn dist.mean()\n",
    "\n",
    "def group_cohesion_test(X, Y, A,perm_n = 1000):\n",
    "    \n",
    "    #test_statistic = sc_weat_effect_size(X, Y, A)\n",
    "    test_statistic = diff_sim(A, X, Y)\n",
    "    jointlist = np.concatenate((X,Y))\n",
    "    permutations = np.array([])\n",
    "\n",
    "    count = 0\n",
    "    cutpoint = len(X)\n",
    "    cutpoint2 = len(Y)\n",
    "    while count < perm_n:\n",
    "        np.random.shuffle(jointlist)\n",
    "        set1 = jointlist[:cutpoint]\n",
    "        set2 = jointlist[cutpoint:]\n",
    "\n",
    "        permutations = np.append(permutations, \n",
    "                                 diff_sim(A, set1, set2)\n",
    "                                 #sc_weat_effect_size(set1, set2, A)\n",
    "        )\n",
    "        count += 1\n",
    "   \n",
    "\n",
    "    P_val = (sum(i <= test_statistic for i in permutations)+1)/(len(permutations)+1)\n",
    "    \n",
    "    perm_mean = np.mean(permutations)\n",
    "    perm_std = np.std(permutations)\n",
    "    t = (perm_mean - test_statistic) / (perm_std/np.sqrt(perm_n))\n",
    "    z = (test_statistic - perm_mean) / (perm_std)\n",
    "    permutations = permutations - perm_mean\n",
    "    sum_c = test_statistic - perm_mean\n",
    "    Pleft = (sum(i <= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "    Pright = (sum(i >= sum_c for i in permutations)+1)/(len(permutations)+1)\n",
    "    Ptot = (sum(abs(i) >= abs(sum_c) for i in permutations)+1)/(len(permutations)+1)\n",
    "    se = np.std(permutations)\n",
    "    return P_val, Ptot, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5cd1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this parameter according to the model you want to use for calculations, putting\n",
    "#a '#' symbol in front of the line, skips that line, for example, if we want \n",
    "#to use twitter embeddings we should put # in front of every other embedding = ... line\n",
    "\n",
    "embedding = common_crawl2\n",
    "#embedding = glove_vectors_google_news\n",
    "#embedding = glove_vectors_wiki\n",
    "#embedding = common_crawl\n",
    "#embedding = glove_vectors_twitter\n",
    "#embedding = glove_vectors_wiki3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "63e848d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we collect top 50 closest words for female and male to represent female \n",
    "#and male as group word lists, top 50 closest words for entrepreneurship attr.\n",
    "#list\n",
    "\n",
    "female = []\n",
    "for word_tuple in embedding.most_similar('female', topn=50):\n",
    "        female.append(word_tuple[0])\n",
    "        \n",
    "male = []\n",
    "for word_tuple in embedding.most_similar('male', topn=50):\n",
    "        male.append(word_tuple[0])\n",
    "        \n",
    "entrepreneurship = []\n",
    "for word_tuple in embedding.most_similar('entrepreneurship', topn=50):\n",
    "        entrepreneurship.append(word_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2aab5419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entrepreneurial',\n",
       " 'innovation',\n",
       " 'entrepreneurs',\n",
       " 'entrepreneur',\n",
       " 'startups',\n",
       " 'leadership',\n",
       " 'philanthropy',\n",
       " 'economics',\n",
       " 'mentoring',\n",
       " 'mba',\n",
       " 'education',\n",
       " 'sustainability',\n",
       " 'start-ups',\n",
       " 'empowerment',\n",
       " 'seminar',\n",
       " 'startup',\n",
       " 'journalism',\n",
       " 'opportunities',\n",
       " 'initiative',\n",
       " 'social',\n",
       " 'creativity',\n",
       " 'business',\n",
       " 'innovators',\n",
       " 'initiatives',\n",
       " 'entrepreneurialism',\n",
       " 'smes',\n",
       " 'competitiveness',\n",
       " 'development',\n",
       " 'mentorship',\n",
       " 'start-up',\n",
       " 'technology',\n",
       " 'excellence',\n",
       " 'literacy',\n",
       " 'ventures',\n",
       " 'nonprofit',\n",
       " 'marketing',\n",
       " 'humanities',\n",
       " 'networking',\n",
       " 'sme',\n",
       " 'success',\n",
       " 'venture',\n",
       " 'psychology',\n",
       " 'undergraduate',\n",
       " 'sustainable',\n",
       " 'graduate',\n",
       " 'finance',\n",
       " 'governance',\n",
       " 'investing',\n",
       " 'partnerships',\n",
       " 'emerging']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entrepreneurship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40a3f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we clean our group word lists and attr word list to remove stopwords\n",
    "female_group_words2 = clean_data(female)\n",
    "male_group_words2 = clean_data(male)\n",
    "entr_words = clean_data(entrepreneurship)\n",
    "\n",
    "#and we collect word embeddings for this group word lists and attr. word list\n",
    "female_word_vectors = collect_word_embeds(female_group_words2, embedding)\n",
    "male_word_vectors = collect_word_embeds(male_group_words2, embedding)\n",
    "entr_word_vectors = collect_word_embeds(entr_words, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c74fa3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28248015"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_weat_score = sc_weat_effect_size(female_word_vectors, male_word_vectors, entr_word_vectors)\n",
    "sc_weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8ad895e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2802765847188419"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_weat_score2 = diff_sim(entr_word_vectors,female_word_vectors,male_word_vectors)\n",
    "sc_weat_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b93688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, ptot,z = group_cohesion_test(female_word_vectors, male_word_vectors, entr_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76ac45ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1918081918081918"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4f0ce08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#personality traits analysis:\n",
    "#this part of the analysis focuses on another approach:\n",
    "# we have the list of personality traits from different sources\n",
    "#(one is http://ideonomy.mit.edu/essays/traits.html),\n",
    "#we calculate their similarity with entrepreneurship according to cosine similarity\n",
    "#and take the words that are more similar\n",
    "def collect_predefined_list_embeddings(wordlist_file, embedding, entrepreneurship_word, top_n):\n",
    "    words = []\n",
    "    with open(wordlist_file) as f:\n",
    "        for line in f:\n",
    "            words.append(line[:-1].lower())\n",
    "            \n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if word in embedding.key_to_index:\n",
    "            word_dict[word] = embedding.similarity(entrepreneurship_word, word)\n",
    "    word_dict2 = sorted(word_dict.items(), key=lambda x:x[1], reverse = True)\n",
    "    new_words = []\n",
    "    for word,score in word_dict2[:top_n]:\n",
    "        new_words.append(word)\n",
    "    word_embeds = collect_word_embeds(new_words, embedding)\n",
    "    return new_words, word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8b015ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_words, word_embeds = collect_predefined_list_embeddings(\"williams-best traits.txt\", embedding, 'entrepreneurship', top_n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e6bef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc__weat_score = sc_weat_effect_size(female_word_vectors, male_word_vectors, word_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e552269f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24471311"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc__weat_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a45d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, ptot,z = group_cohesion_test(female_word_vectors, male_word_vectors, word_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eacd628c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771228771228772"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0d039005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23876123876123875"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2a624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
